# -*- coding: utf-8 -*-
"""one_hot_rnn_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZkoknqPbkILUOEhEQmO_ttuYndYNr-wv
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import nltk
import pandas_profiling as pp
from markupsafe import escape
from wordcloud import WordCloud, STOPWORDS
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import tensorflow as tf
from tensorflow import keras
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split, cross_val_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# model building imports
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU
from keras.models import Sequential
from keras.regularizers import L1L2

# %matplotlib inline

# matplotlib defaults
plt.style.use("ggplot")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

import warnings 
warnings.filterwarnings('ignore')
nltk.download('omw-1.4')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from google.colab import drive
drive.mount('/content/gdrive')

#Data exploration & pre-processing
data = pd.read_json('/content/gdrive/MyDrive/News_Category_Dataset_v2.json', lines=True)
data.head()

# shape of the dataset
print(data.shape)
# total number of unique categories
print("Unique categories:",data['category'].nunique())
print("-------------------------------------------------")
# information about metadata
data.info()

# general descrption of dataset
data.describe().style.set_properties(**{'background-color': '#F0F0F0',
                                    'color': '#222222',
                                    'border': '1.5px  solid black'})

"""1. Dataset has 41 distinct categories of news articles.
2. 'Politics' is the most common category of news in the dataset.
3. There are 27993 unique authors who have written news articles.
"""

# drop columns like authors, links and date as they are irrelevent to the problem.
new_data = data.drop(columns=['authors','link','date'])
new_data.head()

# Top categories by and number of articles per categories
cat_df = pd.DataFrame(new_data['category'].value_counts()).reset_index()
cat_df.rename(columns={'index':'news_classes','category':'numcat'}, inplace=True)

# Visualize top 20 categories and proportion of each categories in dataset
plt.figure(figsize=(10,6))
ax = sns.barplot(np.array(cat_df.news_classes)[:20], np.array(cat_df.numcat)[:20])
for p in ax.patches:
    ax.annotate(p.get_height(), (p.get_x()+0.01, p.get_height() + 50))
plt.title("TOP 20 Categories of News articles", size=15)
plt.xlabel("Categories of articles", size=14)
plt.xticks(rotation=45)
plt.ylabel("Number of articles", size=14)
plt.show()

# plot the pie chart of 41 categories of news articles
fig = plt.figure(figsize=(12,12))
A = plt.pie(cat_df['numcat'][:41],
            labels=cat_df['news_classes'][:41],
            autopct='%1.1f%%',
            startangle=90,
            labeldistance=1.08,
            pctdistance=1.03,
            rotatelabels=45
            )

plt.title("Pie Chart of 41 categories of news articles", size=20, weight='bold')
plt.show()

new_data=new_data[new_data.category.isin(['BUSINESS', 'ENTERTAINMENT','POLITICS','SPORTS','TECH'])]

"""We will only use 5 categories news among 41 unique categories:
1. business: 5937 (3.0%)
2. entertainment: 16058 (8.0%)
3. politics: 32739 (16.3%)
4. sports: 4884 (2.4%)
5. tech: 2082 (1.0%)
"""

# create final dataframe of combined headline and short_description
final_data = new_data.copy()
final_data['length_of_news'] = final_data['headline'] + final_data['short_description']
final_data.drop(['headline','short_description'], inplace=True, axis=1)
final_data['len_news'] = final_data['length_of_news'].map(lambda x: len(x))
final_data.head()

# maximum length of news in each category
lenmax_df = final_data.groupby('category')['len_news'].max().reset_index().sort_values(by='len_news',ascending=False)
lenmax_df.head()

# minimum lenght of news in each category
lenmin_df = final_data.groupby('category')['len_news'].min().reset_index().sort_values(by='len_news',ascending=False)
lenmin_df.head()

# plot the bar plots of max and min length of news articles
plt.figure(figsize=(15,6))

plt.subplot(121)
plt.bar(lenmax_df['category'][:5],lenmax_df['len_news'][:5])
plt.xticks(rotation=45)
plt.ylabel("Words length in news categories", size=14)
plt.xlabel("News categories",  size=14)
plt.text(6.5,1400, '''       POLITICS HAS 
 THE LONGEST ARTICLE''', fontsize=12)
plt.title("Max length 5 news categories", size=14)

plt.subplot(122)
plt.bar(lenmin_df['category'][:5], lenmin_df['len_news'][:5])
plt.xticks(rotation=45)
plt.xlabel("News categories", size=14)
plt.title("Min length of 5 news categories", size=14)

plt.show()

"""1. POLITICS has the most number of articles as well as length of characters in the article.
2. After 'POLITICS' category, 'BUSINESS' and 'ENTERTAINMENT' are in top 3 most length of articles.
3. 'POLITICS', 'BUSINESS' and 'ENTERTAINMENT' are top 3 categories having most number of articles in the dataset.
"""

# wordcloud of categories of news articles in the dataset
plt.figure(figsize=(12,12))
wc = WordCloud(max_words=1000, 
               min_font_size=10,
               height=600,
               width=1600,
               background_color='black',
               contour_color='black',
               colormap='plasma',
               repeat=False,
               stopwords=STOPWORDS).generate(' '.join(final_data.category))

plt.title("5 News Categories' Wordcloud", size=15, weight='bold')
plt.imshow(wc, interpolation= "bilinear")
plt.axis('off')

ndf = final_data.copy()
ndf.drop('len_news', inplace=True, axis=1)
print(ndf)

#Text data preprocessing
# clean the text data using regex and data cleaning function
def datacleaning(text):
    whitespace = re.compile(r"\s+")
    user = re.compile(r"(?i)@[a-z0-9_]+")
    text = whitespace.sub(' ', text)
    text = user.sub('', text)
    text = re.sub(r"\[[^()]*\]","", text)
    text = re.sub("\d+", "", text)
    text = re.sub(r'[^\w\s]','',text)
    text = re.sub(r"(?:@\S*|#\S*|http(?=.*://)\S*)", "", text)
    text = text.lower()
    
    # removing stop-words
    text = [word for word in text.split() if word not in list(STOPWORDS)]
    
    # word lemmatization
    sentence = []
    for word in text:
        lemmatizer = WordNetLemmatizer()
        sentence.append(lemmatizer.lemmatize(word,'v'))
        
    return ' '.join(sentence)

# Example of pre-processing using above function
import nltk
nltk.download('wordnet')
print("Text sentence before pre-processing:\n",ndf['length_of_news'][1])
print("---"*35)
print("Text sentence after pre-processing:\n",datacleaning(ndf['length_of_news'][1]))

# apply datacleaning function to column 'length_of_news'
ndf['length_of_news'] = ndf['length_of_news'].apply(datacleaning)

# length of total characters
original_length = ndf['length_of_news'].apply(len).sum()
print(f"chracters length of text data: {original_length}")

# length of total words
original_text = ndf['length_of_news'].apply(lambda x: len([i for i in x.split()])).sum()
print(f"word length of text data: {original_text}")

# tokenization and vectorization workflow
# word level one-hot encoding for sample data

samples = list(ndf['length_of_news'][:5].values)  # samples of first 5 documents of out dataset

token_index = {}  # builds an index of tokens in the data
for sample in samples:
    for word in sample.split():
        if word not in token_index:
            token_index[word] = len(token_index) + 1 # assigning unique index to each unique words
            
max_length = 15

results = np.zeros(shape=(len(samples),   # results will be stored in this array
                          max_length,
                          max(token_index.values()) +1)) 

print("Shape of stored results array:", results.shape)
print("Token index of unique words: \n", token_index)

for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        index = token_index.get(word)
        results[i,j,index] = 1

# one hot encoding using keras tokenizer and pad sequencing
X = ndf['length_of_news']
encoder = LabelEncoder()
y = encoder.fit_transform(ndf['category'])
print("shape of input data: ", X.shape)
print("shape of target variable: ", y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

tokenizer = Tokenizer(num_words=100000, oov_token='<00V>') 
tokenizer.fit_on_texts(X_train) # build the word index
# padding X_train text input data
train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists
train_padseq = pad_sequences(train_seq, maxlen=100) # pads the integer lists to 2D integer tensor 

# padding X_test text input data
test_seq = tokenizer.texts_to_sequences(X_test)
test_padseq = pad_sequences(test_seq, maxlen=100)

word_index = tokenizer.word_index
max_words = 150000  # total number of words to consider in embedding layer
total_words = len(word_index)
maxlen = 100 # max length of sequence 
y_train = to_categorical(y_train, num_classes=5)
y_test = to_categorical(y_test, num_classes=5)
print("Length of word index:", total_words)

# basline model using embedding layers and simpleRNN
model = Sequential()
model.add(Embedding(total_words+1, 70, input_length=maxlen))
model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))
model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))
model.add(SimpleRNN(32, activation='tanh'))
model.add(Dropout(0.2))
model.add(Dense(5, activation='softmax'))
model.summary()

model.compile(optimizer='rmsprop',
            loss='categorical_crossentropy',
            metrics=['accuracy']
            )
# SETUP A EARLY STOPPING CALL and model check point API
earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',
                                             patience=5,
                                              verbose=1,
                                              mode='min'
                                             )
checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)
callback_list = [checkpointer, earlystopping]

# fit model to the data
history = model.fit(train_padseq, y_train, 
                   batch_size=128, 
                    epochs=15, 
                    validation_split=0.2
                   )

# evalute the model
test_loss, test_acc = model.evaluate(test_padseq, y_test, verbose=0)
print("test loss and accuracy:", test_loss, test_acc)

# Save the entire model to a HDF5 file.
# The '.h5' extension indicates that the model should be saved to HDF5.
model.save('one_hot_rnn1.h5')

from google.colab import files
files.download("one_hot_rnn1.h5")

# load and evaluate a saved model
from numpy import loadtxt
from tensorflow.keras.models import load_model
 
# load model
onehot_rnn_model_test = load_model('one_hot_rnn1.h5')
# summarize model.
onehot_rnn_model_test.summary()

tokenizer = Tokenizer(num_words=100000, oov_token='<00V>') 
tokenizer.fit_on_texts(X_train) # build the word index
test_text1 = "The earth is an great place live"
test_text2 = "The is my program"
sequences = tokenizer.texts_to_sequences([test_text1, test_text2])
sample_train_padseq = pad_sequences(sequences, maxlen=100)

sample_train_padseq.shape

prediction = onehot_rnn_model_test.predict(np.stack(sample_train_padseq))
output = np.argmax(prediction, axis=1)
category_output = encoder.inverse_transform(output)
print('Output:\n',category_output)
