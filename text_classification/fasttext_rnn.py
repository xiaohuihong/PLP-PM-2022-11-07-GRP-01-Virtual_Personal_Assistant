# -*- coding: utf-8 -*-
"""fasttext_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VX2AdA26gfM-2B2zBrAt7zfUTBxm7GPT
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import nltk
import pandas_profiling as pp
from markupsafe import escape
from wordcloud import WordCloud, STOPWORDS
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import tensorflow as tf
from tensorflow import keras
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split, cross_val_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# model building imports
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU
from keras.models import Sequential
from keras.regularizers import L1L2
from sklearn.metrics import classification_report, accuracy_score
# %matplotlib inline

# matplotlib defaults
plt.style.use("ggplot")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

import warnings 
warnings.filterwarnings('ignore')
nltk.download('omw-1.4')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# fastext
from gensim.models import FastText

from tabulate import tabulate

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/gdrive')

#Data exploration & pre-processing
data = pd.read_json('/content/gdrive/MyDrive/News_Category_Dataset_v2.json', lines=True)
data.head()

# shape of the dataset
print(data.shape)
# total number of unique categories
print("Unique categories:",data['category'].nunique())
print("-------------------------------------------------")
# information about metadata
data.info()

# general descrption of dataset
data.describe().style.set_properties(**{'background-color': '#F0F0F0',
                                    'color': '#222222',
                                    'border': '1.5px  solid black'})

"""1. Dataset has 41 distinct categories of news articles.
2. 'Politics' is the most common category of news in the dataset.
3. There are 27993 unique authors who have written news articles.
"""

# drop columns like authors, links and date as they are irrelevent to the problem.
new_data = data.drop(columns=['authors','link','date'])
new_data.head()

# Top categories by and number of articles per categories
cat_df = pd.DataFrame(new_data['category'].value_counts()).reset_index()
cat_df.rename(columns={'index':'news_classes','category':'numcat'}, inplace=True)

# Visualize top 20 categories and proportion of each categories in dataset
plt.figure(figsize=(10,6))
ax = sns.barplot(np.array(cat_df.news_classes)[:20], np.array(cat_df.numcat)[:20])
for p in ax.patches:
    ax.annotate(p.get_height(), (p.get_x()+0.01, p.get_height() + 50))
plt.title("TOP 20 Categories of News articles", size=15)
plt.xlabel("Categories of articles", size=14)
plt.xticks(rotation=45)
plt.ylabel("Number of articles", size=14)
plt.show()

# plot the pie chart of 41 categories of news articles
fig = plt.figure(figsize=(12,12))
A = plt.pie(cat_df['numcat'][:41],
            labels=cat_df['news_classes'][:41],
            autopct='%1.1f%%',
            startangle=90,
            labeldistance=1.08,
            pctdistance=1.03,
            rotatelabels=45
            )

plt.title("Pie Chart of 41 categories of news articles", size=20, weight='bold')
plt.show()

new_data=new_data[new_data.category.isin(['BUSINESS', 'ENTERTAINMENT','POLITICS','SPORTS','TECH'])]

"""We will only use 5 categories news among 41 unique categories:
1. business: 5937 (3.0%)
2. entertainment: 16058 (8.0%)
3. politics: 32739 (16.3%)
4. sports: 4884 (2.4%)
5. tech: 2082 (1.0%)
"""

# create final dataframe of combined headline and short_description
final_data = new_data.copy()
final_data['length_of_news'] = final_data['headline'] + final_data['short_description']
final_data.drop(['headline','short_description'], inplace=True, axis=1)
final_data['len_news'] = final_data['length_of_news'].map(lambda x: len(x))
final_data.head()

# maximum length of news in each category
lenmax_df = final_data.groupby('category')['len_news'].max().reset_index().sort_values(by='len_news',ascending=False)
lenmax_df.head()

# minimum lenght of news in each category
lenmin_df = final_data.groupby('category')['len_news'].min().reset_index().sort_values(by='len_news',ascending=False)
lenmin_df.head()

# plot the bar plots of max and min length of news articles
plt.figure(figsize=(15,6))

plt.subplot(121)
plt.bar(lenmax_df['category'][:5],lenmax_df['len_news'][:5])
plt.xticks(rotation=45)
plt.ylabel("Words length in news categories", size=14)
plt.xlabel("News categories",  size=14)
plt.text(6.5,1400, '''       POLITICS HAS 
 THE LONGEST ARTICLE''', fontsize=12)
plt.title("Max length 5 news categories", size=14)

plt.subplot(122)
plt.bar(lenmin_df['category'][:5], lenmin_df['len_news'][:5])
plt.xticks(rotation=45)
plt.xlabel("News categories", size=14)
plt.title("Min length of 5 news categories", size=14)

plt.show()

"""1. POLITICS has the most number of articles as well as length of characters in the article.
2. After 'POLITICS' category, 'BUSINESS' and 'ENTERTAINMENT' are in top 3 most length of articles.
3. 'POLITICS', 'BUSINESS' and 'ENTERTAINMENT' are top 3 categories having most number of articles in the dataset.
"""

# wordcloud of categories of news articles in the dataset
plt.figure(figsize=(12,12))
wc = WordCloud(max_words=1000, 
               min_font_size=10,
               height=600,
               width=1600,
               background_color='black',
               contour_color='black',
               colormap='plasma',
               repeat=False,
               stopwords=STOPWORDS).generate(' '.join(final_data.category))

plt.title("5 News Categories' Wordcloud", size=15, weight='bold')
plt.imshow(wc, interpolation= "bilinear")
plt.axis('off')

ndf = final_data.copy()
ndf.drop('len_news', inplace=True, axis=1)
print(ndf)

#Counting by Subjects 
for key,count in ndf.category.value_counts().iteritems():
    print(f"{key}:\t{count}")
    
#Getting Total Rows
print(f"Total Records:\t{ndf.shape[0]}")

plt.figure(figsize=(8,5))
sns.countplot("category", data=ndf)
plt.show()

#Word Cloud
import nltk
nltk.download('stopwords')
length_of_news = ''
for news in ndf.length_of_news.values:
    length_of_news += f" {news}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(length_of_news)
fig = plt.figure(
    figsize = (8, 5),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del length_of_news

import nltk
nltk.download('punkt')
y = ndf["category"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in ndf["length_of_news"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X.append(tmp)

del ndf

import gensim

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by FastText Method
w2v_model = gensim.models.FastText(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)

w2v_model.save('fasttext_trained_model')

fasttext_model = gensim.models.FastText.load('fasttext_trained_model')

# Retrieve the weights from the model. This is used for initializing the weights
# in a Keras Embedding layer later

ft_weights = fasttext_model.wv.vectors

vocab_size, embedding_size = ft_weights.shape
print("Vocabulary Size: {} - Embedding Dim: {}".format(vocab_size, embedding_size))

#vocab size
len(w2v_model.wv.vocab)

#We have now represented each of 75863 words by a 100dim vector.

#sample vector for random word, lets say people 
fasttext_model["people"]

fasttext_model.wv.most_similar("america")

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer

#Tokenized applies basic processing like changing it to lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

# Check the first 10 words of first news
# Every word has been represented with a number
X[0][:10]

#Check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 10:
        break

# For determining size of input

# Making histogram for no of words in news shows that most news article are under 700 words.
# Lets keep each news small and truncate all news to 60 while tokenizing
plt.hist([len(x) for x in X], bins=500)
plt.show()

# Its heavily skewed

nos = np.array([len(x) for x in X])
#len(nos)
len(nos[nos  < 60])
# Out of 61700 news, 61574 have less than 60 words

#Lets keep all news to 60, add padding to news with less than 60 words and truncating long ones
maxlen = 60 

#Making all news of size maxlen defined above
X = pad_sequences(X, maxlen=maxlen)

#all news has 60 words (in numerical form now). If they had less words, they have been padded with 0
len(X[0])

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

#Getting embedding vectors from fasttext and usings it as weights of non-trainable keras embedding layer
embedding_vectors = get_weight_matrix(fasttext_model, word_index)

# basline model using embedding layers and simpleRNN
model = Sequential()
model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen,trainable=False))
model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))
model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))
model.add(SimpleRNN(32, activation='tanh'))
model.add(Dropout(0.2))
model.add(Dense(5, activation='softmax'))
model.summary()

encoder = LabelEncoder()
y = encoder.fit_transform(y)

#Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y)

y_train = to_categorical(y_train, num_classes=5)
y_test = to_categorical(y_test, num_classes=5)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

model.compile(optimizer='rmsprop',
            loss='categorical_crossentropy',
            metrics=['accuracy']
            )
# SETUP A EARLY STOPPING CALL and model check point API
earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',
                                             patience=5,
                                              verbose=1,
                                              mode='min'
                                             )
checkpointer = ModelCheckpoint(filepath='bestvalue',moniter='val_loss', verbose=0, save_best_only=True)
callback_list = [checkpointer, earlystopping]

# fit model to the data
history = model.fit(X_train, y_train, 
                   batch_size=128, 
                    epochs=15, 
                    validation_split=0.2
                   )

# evalute the model
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print("test loss and accuracy:", test_loss, test_acc)

model2 = Sequential()
model2.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen,trainable=False))
model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))
model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))
model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))
model2.add(Conv1D(76, 3, activation='relu'))
model2.add(MaxPooling1D(2))
model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))
model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))
model2.add(Dropout(0.2))
model2.add(Dense(5, activation='softmax'))
model2.summary()

model2.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy']
              )
# SETUP A EARLY STOPPING CALL and model check point API
earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',
                                              patience=5,
                                              verbose=1,
                                              mode='min'
                                              )
checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)
callback_list = [checkpointer, earlystopping]

# fit model to the data
history2 = model2.fit(X_train, y_train, 
                     batch_size=128, 
                     epochs=15, 
                     validation_split=0.2,
                     shuffle=True
                    )

# evalute the model
test_loss2, test_acc2 = model2.evaluate(X_test, y_test, verbose=0)
print("test loss and accuracy:", test_loss2, test_acc2)

plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model 1 accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
plt.show()

plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model 1 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
plt.show()

#Evaluate model 1
from sklearn.metrics import confusion_matrix
import itertools

predicted_labels = model.predict(np.stack(X_test))
cm1 = confusion_matrix(np.argmax(y_test, axis=1), 
                      np.argmax(predicted_labels, axis=1))
print('Confusion matrix:\n',cm1)

cm1 = cm1.astype('float') / cm1.sum(axis = 1)[:, np.newaxis]

plt.imshow(cm1, cmap=plt.cm.Blues)
plt.title('Normalized confusion matrix')
plt.colorbar()
plt.xlabel('True label')
plt.ylabel('Predicted label')
plt.xticks([0, 4]); plt.yticks([0, 4])
plt.grid('off')
for i, j in itertools.product(range(cm1.shape[0]), range(cm1.shape[1])):
    plt.text(j, i, format(cm1[i, j], '.2f'),
             horizontalalignment='center',
             color='white' if cm1[i, j] > 0.5 else 'black')

total=sum(sum(cm1))
accuracy=(cm1[0,0]+cm1[1,1]+cm1[2,2]+cm1[3,3]+cm1[4,4])/total
print ('Accuracy : ', accuracy)

sensitivity_business = cm1[0,0]/(cm1[0,0]+cm1[1,0]+cm1[2,0]+cm1[3,0]+cm1[4,0])
print('Sensitivity for business : ', sensitivity_business )

sensitivity_entertainment = cm1[1,1]/(cm1[1,1]+cm1[0,1]+cm1[2,1]+cm1[3,1]+cm1[4,1])
print('Sensitivity for entertainment : ', sensitivity_entertainment )

sensitivity_politics = cm1[2,2]/(cm1[2,2]+cm1[0,2]+cm1[1,2]+cm1[3,2]+cm1[4,2])
print('Sensitivity for politics : ', sensitivity_politics )

sensitivity_sports = cm1[3,3]/(cm1[3,3]+cm1[0,3]+cm1[1,3]+cm1[2,3]+cm1[4,3])
print('Sensitivity for sports : ', sensitivity_sports )

sensitivity_tech = cm1[4,4]/(cm1[4,4]+cm1[0,4]+cm1[1,4]+cm1[2,4]+cm1[3,4])
print('Sensitivity for tech : ', sensitivity_tech )

plt.figure()
plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('model 2 accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
plt.show()

plt.figure()
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model 2 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
plt.show()

#Evaluate model 2
from sklearn.metrics import confusion_matrix
import itertools

predicted_labels = model2.predict(np.stack(X_test))
cm2 = confusion_matrix(np.argmax(y_test, axis=1), 
                      np.argmax(predicted_labels, axis=1))
print('Confusion matrix:\n',cm2)

cm2 = cm2.astype('float') / cm2.sum(axis = 1)[:, np.newaxis]

plt.imshow(cm2, cmap=plt.cm.Blues)
plt.title('Normalized confusion matrix')
plt.colorbar()
plt.xlabel('True label')
plt.ylabel('Predicted label')
plt.xticks([0, 4]); plt.yticks([0, 4])
plt.grid('off')
for i, j in itertools.product(range(cm2.shape[0]), range(cm2.shape[1])):
    plt.text(j, i, format(cm2[i, j], '.2f'),
             horizontalalignment='center',
             color='white' if cm2[i, j] > 0.5 else 'black')

total=sum(sum(cm2))
accuracy=(cm2[0,0]+cm2[1,1]+cm2[2,2]+cm2[3,3]+cm2[4,4])/total
print ('Accuracy : ', accuracy)

sensitivity_business = cm2[0,0]/(cm2[0,0]+cm2[1,0]+cm2[2,0]+cm2[3,0]+cm2[4,0])
print('Sensitivity for business : ', sensitivity_business )

sensitivity_entertainment = cm2[1,1]/(cm2[1,1]+cm2[0,1]+cm2[2,1]+cm2[3,1]+cm2[4,1])
print('Sensitivity for entertainment : ', sensitivity_entertainment )

sensitivity_politics = cm2[2,2]/(cm2[2,2]+cm2[0,2]+cm2[1,2]+cm2[3,2]+cm2[4,2])
print('Sensitivity for politics : ', sensitivity_politics )

sensitivity_sports = cm2[3,3]/(cm2[3,3]+cm2[0,3]+cm2[1,3]+cm2[2,3]+cm2[4,3])
print('Sensitivity for sports : ', sensitivity_sports )

sensitivity_tech = cm2[4,4]/(cm2[4,4]+cm2[0,4]+cm2[1,4]+cm2[2,4]+cm2[3,4])
print('Sensitivity for tech : ', sensitivity_tech )