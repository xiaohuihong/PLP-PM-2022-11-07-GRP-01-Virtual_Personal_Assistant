{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e314a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0efd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  University_of_Notre_Dame\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Super_Bowl_50\n",
      "Train list len:  87599\n",
      "Test list len:  34726\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset json files\n",
    "\n",
    "train_data = load_json('./dataset/squad_train.json')\n",
    "test_data = load_json('./dataset/squad_test.json')\n",
    "\n",
    "# parse the json structure to return the data as a list of dictionaries\n",
    "\n",
    "train_list = parse_data(train_data)\n",
    "test_list = parse_data(test_data)\n",
    "\n",
    "print('Train list len: ',len(train_list))\n",
    "print('Test list len: ',len(test_list))\n",
    "\n",
    "# converting the lists into dataframes\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "test_df = pd.DataFrame(test_list)\n",
    "\n",
    "def normalize_spaces(text):\n",
    "    '''\n",
    "    Removes extra white spaces from the context.\n",
    "    '''\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "train_df.context = train_df.context.apply(normalize_spaces)\n",
    "test_df.context = test_df.context.apply(normalize_spaces)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of outliers and drop them from the dataframe\n",
    "\n",
    "drop_ids_train = filter_large_examples(train_df)\n",
    "train_df.drop(list(drop_ids_train), inplace=True)\n",
    "\n",
    "drop_ids_test = filter_large_examples(test_df)\n",
    "test_df.drop(list(drop_ids_test), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c29bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c54b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather text to build vocabularies\n",
    "\n",
    "vocab_text = gather_text_for_vocab([train_df, test_df])\n",
    "print(\"Number of sentences in dataset: \", len(vocab_text))\n",
    "vocab_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f907283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word and character-level vocabularies\n",
    "print(\"---------------word vocabulary-------------------\")\n",
    "word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n",
    "print(\"------------character vocabulary-----------------\")\n",
    "char2idx, char_vocab = build_char_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./dataset/qa_word2idx.npy',word2idx)\n",
    "np.save('./dataset/qa_idx2word.npy',idx2word)\n",
    "np.save('./dataset/qa_word_vocab.npy',word_vocab)\n",
    "np.save('./dataset/qa_char2idx.npy',char2idx)\n",
    "np.save('./dataset/qa_char_vocab.npy',char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize context and questions for training and testing set\n",
    "\n",
    "\n",
    "train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "test_df['context_ids'] = test_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "\n",
    "train_df['question_ids'] = train_df.question.apply(question_to_ids,  word2idx=word2idx)\n",
    "test_df['question_ids'] = test_df.question.apply(question_to_ids,  word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "\n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "test_err = get_error_indices(test_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "test_df.drop(test_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0651d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "test_label_idx = test_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "test_df['label_idx'] = test_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fe98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump data to pickle files\n",
    "import pickle\n",
    "with open('./dataset/qaw2id.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "\n",
    "with open('./dataset/qac2id.pickle','wb') as handle:\n",
    "    pickle.dump(char2idx, handle)\n",
    "    \n",
    "train_df.to_pickle('./dataset/qatrain.pkl')\n",
    "test_df.to_pickle('./dataset/qatest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_matrix():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(\"./dataset/glove.840B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = vector\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict\n",
    "glove_dict = create_glove_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df38f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_embedding(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 300))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "    return weights_matrix, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23980fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix, words_found = create_word_embedding(glove_dict)\n",
    "print(\"Total words found in glove vocab: \", words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./dataset/qaglove_vt.npy',weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b77e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
