{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4a3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from preprocess import *\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = './dataset/squad_test.json'\n",
    "model_path = './model/model_encoder_transformer.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4124db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed data from pickle\n",
    "train_df = pd.read_pickle('./dataset/qatrain.pkl')\n",
    "test_df = pd.read_pickle('./dataset/qatest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c955a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.load('./dataset/qaglove_vt.npy')\n",
    "word2idx = np.load('./dataset/qa_word2idx.npy', allow_pickle=True).item()\n",
    "idx2word = np.load('./dataset/qa_idx2word.npy', allow_pickle=True).item()\n",
    "char2idx = np.load('./dataset/qa_char2idx.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8b0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    - Creates batches dynamically by padding to the length of largest example\n",
    "      in a given batch.\n",
    "    - Calulates character vectors for contexts and question.\n",
    "    - Returns tensors for training.\n",
    "    '''\n",
    "    def __init__(self, data, batch_size):\n",
    "        '''\n",
    "        data: dataframe\n",
    "        batch_size: int\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, sentence, max_word_len=16):\n",
    "        \n",
    "        char_vec = torch.zeros(max_sent_len, max_word_len).type(torch.LongTensor)\n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                if j == max_word_len:\n",
    "                    break\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)\n",
    "        \n",
    "        return char_vec     \n",
    "    \n",
    "    def get_span(self, text):\n",
    "\n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :char_ctx & ques_ctx: character-level ids for context and question\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while testing to calculate metrics\n",
    "        :ids: question_ids for evaluation\n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "            \n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "             \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "                \n",
    "            max_word_ctx = 16\n",
    "          \n",
    "            char_ctx = torch.zeros(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, context)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 16\n",
    "            \n",
    "            char_ques = torch.zeros(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, question)\n",
    "            \n",
    "              \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            ids = list(batch.id)\n",
    "            \n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee6d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataset = SquadDataset(train_df,16)\n",
    "test_dataset = SquadDataset(test_df,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79eebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if dim == 2:\n",
    "            \n",
    "            self.depthwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels,\n",
    "                                        kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2)\n",
    "        \n",
    "            self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    \n",
    "        else:\n",
    "        \n",
    "            self.depthwise_conv = nn.Conv1d(in_channels=in_channels, out_channels=in_channels,\n",
    "                                            kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2,\n",
    "                                            bias=False)\n",
    "\n",
    "            self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, emb_dim]\n",
    "        if self.dim == 1:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.pointwise_conv(self.depthwise_conv(x))\n",
    "            x = x.transpose(1,2)\n",
    "        else:\n",
    "            x = self.pointwise_conv(self.depthwise_conv(x))\n",
    "        #print(\"DepthWiseConv output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b589e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_dim, num_layers=2):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layers = nn.ModuleList([nn.Linear(layer_dim, layer_dim) for _ in range(num_layers)])\n",
    "        self.gate_layers = nn.ModuleList([nn.Linear(layer_dim, layer_dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"Highway input: \", x.shape)\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow = self.flow_layers[i](x)\n",
    "            gate = torch.sigmoid(self.gate_layers[i](x))\n",
    "            \n",
    "            x = gate * flow + (1 - gate) * x\n",
    "            \n",
    "        #print(\"Highway output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e245e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, kernel_size, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim)\n",
    "        \n",
    "        self.word_embedding = self.get_glove_word_embedding()\n",
    "        \n",
    "        self.conv2d = DepthwiseSeparableConvolution(char_emb_dim, char_emb_dim, kernel_size,dim=2)\n",
    "        \n",
    "        self.highway = HighwayLayer(self.word_emb_dim + char_emb_dim)\n",
    "    \n",
    "        \n",
    "    def get_glove_word_embedding(self):\n",
    "        \n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.word_emb_dim = embedding_dim\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x, x_char):\n",
    "        # x = [bs, seq_len]\n",
    "        # x_char = [bs, seq_len, word_len(=16)]\n",
    "        \n",
    "        word_emb = self.word_embedding(x)\n",
    "        # word_emb = [bs, seq_len, word_emb_dim]\n",
    "        \n",
    "        word_emb = F.dropout(word_emb,p=0.1)\n",
    "        \n",
    "        char_emb = self.char_embedding(x_char)\n",
    "        # char_embed = [bs, seq_len, word_len, char_emb_dim]\n",
    "        \n",
    "        char_emb = F.dropout(char_emb.permute(0,3,1,2), p=0.05)\n",
    "        # [bs, char_emb_dim, seq_len, word_len] == [N, Cin, Hin, Win]\n",
    "        \n",
    "        conv_out = F.relu(self.conv2d(char_emb))\n",
    "        # [bs, char_emb_dim, seq_len, word_len] \n",
    "        # the depthwise separable conv does not change the shape of the input\n",
    "        \n",
    "        char_emb, _ = torch.max(conv_out, dim=3)\n",
    "        # [bs, char_emb_dim, seq_len]\n",
    "        \n",
    "        char_emb = char_emb.permute(0,2,1)\n",
    "        # [bs, seq_len, char_emb_dim]\n",
    "        \n",
    "        concat_emb = torch.cat([char_emb, word_emb], dim=2)\n",
    "        # [bs, seq_len, char_emb_dim + word_emb_dim]\n",
    "        \n",
    "        emb = self.highway(concat_emb)\n",
    "        # [bs, seq_len, char_emb_dim + word_emb_dim]\n",
    "        \n",
    "        #print(\"Embedding output: \", emb.shape)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad294877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, num_heads, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.head_dim = self.hid_dim // self.num_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x = [bs, len_x, hid_dim]\n",
    "        # mask = [bs, len_x]\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(x)\n",
    "        K = self.fc_k(x)\n",
    "        V = self.fc_v(x)\n",
    "        # Q = K = V = [bs, len_x, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        # [bs, len_x, num_heads, head_dim ]  => [bs, num_heads, len_x, head_dim]\n",
    "        \n",
    "        K = K.permute(0,1,3,2)\n",
    "        # [bs, num_heads, head_dim, len_x]\n",
    "        \n",
    "        energy = torch.matmul(Q, K) / self.scale\n",
    "        # (bs, num_heads){[len_x, head_dim] * [head_dim, len_x]} => [bs, num_heads, len_x, len_x]\n",
    "        \n",
    "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        # [bs, 1, 1, len_x]\n",
    "        \n",
    "        #print(\"Mask: \", mask)\n",
    "        #print(\"Energy: \", energy)\n",
    "        \n",
    "        energy = energy.masked_fill(mask == 1, -1e10)\n",
    "        \n",
    "        #print(\"energy after masking: \", energy)\n",
    "        \n",
    "        alpha = torch.softmax(energy, dim=-1)\n",
    "        #  [bs, num_heads, len_x, len_x]\n",
    "        \n",
    "        #print(\"energy after smax: \", alpha)\n",
    "        alpha = F.dropout(alpha, p=0.1)\n",
    "        \n",
    "        a = torch.matmul(alpha, V)\n",
    "        # [bs, num_heads, len_x, head_dim]\n",
    "        \n",
    "        a = a.permute(0,2,1,3)\n",
    "        # [bs, len_x, num_heads, hid_dim]\n",
    "        \n",
    "        a = a.contiguous().view(batch_size, -1, self.hid_dim)\n",
    "        # [bs, len_x, hid_dim]\n",
    "        \n",
    "        a = self.fc_o(a)\n",
    "        # [bs, len_x, hid_dim]\n",
    "        \n",
    "        #print(\"Multihead output: \", a.shape)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dda8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, device, max_length=400):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_length, model_dim)\n",
    "        \n",
    "        for pos in range(max_length):\n",
    "            \n",
    "            for i in range(0, model_dim, 2):\n",
    "                \n",
    "                pos_encoding[pos, i] = math.sin(pos / (10000 ** ((2*i)/model_dim)))\n",
    "                pos_encoding[pos, i+1] = math.cos(pos / (10000 ** ((2*(i+1))/model_dim)))\n",
    "            \n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).to(device)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"PE shape: \", self.pos_encoding.shape)\n",
    "        #print(\"PE input: \", x.shape)\n",
    "        x = x + Variable(self.pos_encoding[:, :x.shape[1]], requires_grad=False)\n",
    "        #print(\"PE output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4063734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, num_heads, num_conv_layers, kernel_size, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList([DepthwiseSeparableConvolution(model_dim, model_dim, kernel_size)\n",
    "                                          for _ in range(num_conv_layers)])\n",
    "        \n",
    "        self.multihead_self_attn = MultiheadAttentionLayer(model_dim, num_heads, device)\n",
    "        \n",
    "        self.position_encoder = PositionEncoder(model_dim, device)\n",
    "        \n",
    "        self.pos_norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.conv_norm = nn.ModuleList([nn.LayerNorm(model_dim) for _ in range(self.num_conv_layers)])\n",
    "        \n",
    "        self.feedfwd_norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.feed_fwd = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x = [bs, len_x, model_dim]\n",
    "        # mask = [bs, len_x]\n",
    "        \n",
    "        out = self.position_encoder(x)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        res = out\n",
    "        \n",
    "        out = self.pos_norm(out)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            \n",
    "            out = F.relu(conv_layer(out))\n",
    "            out = out + res\n",
    "            if (i+1) % 2 == 0:\n",
    "                out = F.dropout(out, p=0.1)\n",
    "            res = out\n",
    "            out = self.conv_norm[i](out)\n",
    "        \n",
    "        \n",
    "        out = self.multihead_self_attn(out, mask)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        out = F.dropout(out + res, p=0.1)\n",
    "        \n",
    "        res = out\n",
    "        \n",
    "        out = self.feedfwd_norm(out)\n",
    "        \n",
    "        out = F.relu(self.feed_fwd(out))\n",
    "        # [bs, len_x, model_dim]\n",
    "            \n",
    "        out = F.dropout(out + res, p=0.1)\n",
    "        # [bs, len_x, model_dim]\n",
    "        #print(\"Encoder block output: \", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1ed6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQueryAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.W0 = nn.Linear(3*model_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, C, Q, c_mask, q_mask):\n",
    "        # C = [bs, ctx_len, model_dim]\n",
    "        # Q = [bs, qtn_len, model_dim]\n",
    "        # c_mask = [bs, ctx_len]\n",
    "        # q_mask = [bs, qtn_len]\n",
    "        \n",
    "        c_mask = c_mask.unsqueeze(2)\n",
    "        # [bs, ctx_len, 1]\n",
    "        \n",
    "        q_mask = q_mask.unsqueeze(1)\n",
    "        # [bs, 1, qtn_len]\n",
    "        \n",
    "        ctx_len = C.shape[1]\n",
    "        qtn_len = Q.shape[1]\n",
    "        \n",
    "        C_ = C.unsqueeze(2).repeat(1,1,qtn_len,1)\n",
    "        # [bs, ctx_len, qtn_len, model_dim] \n",
    "        \n",
    "        Q_ = Q.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        # [bs, ctx_len, qtn_len, model_dim]\n",
    "        \n",
    "        C_elemwise_Q = torch.mul(C_, Q_)\n",
    "        # [bs, ctx_len, qtn_len, model_dim]\n",
    "        \n",
    "        S = torch.cat([C_, Q_, C_elemwise_Q], dim=3)\n",
    "        # [bs, ctx_len, qtn_len, model_dim*3]\n",
    "        \n",
    "        S = self.W0(S).squeeze()\n",
    "        #print(\"Simi matrix: \", S.shape)\n",
    "        # [bs, ctx_len, qtn_len, 1] => # [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        S_row = S.masked_fill(q_mask==1, -1e10)\n",
    "        S_row = F.softmax(S_row, dim=2)\n",
    "        \n",
    "        S_col = S.masked_fill(c_mask==1, -1e10)\n",
    "        S_col = F.softmax(S_col, dim=1)\n",
    "        \n",
    "        A = torch.bmm(S_row, Q)\n",
    "        # (bs)[ctx_len, qtn_len] X [qtn_len, model_dim] => [bs, ctx_len, model_dim]\n",
    "        \n",
    "        B = torch.bmm(torch.bmm(S_row,S_col.transpose(1,2)), C)\n",
    "        # [ctx_len, qtn_len] X [qtn_len, ctx_len] => [bs, ctx_len, ctx_len]\n",
    "        # [ctx_len, ctx_len] X [ctx_len, model_dim ] => [bs, ctx_len, model_dim]\n",
    "        \n",
    "        model_out = torch.cat([C, A, torch.mul(C,A), torch.mul(C,B)], dim=2)\n",
    "        # [bs, ctx_len, model_dim*4]\n",
    "        \n",
    "        #print(\"C2Q output: \", model_out.shape)\n",
    "        return F.dropout(model_out, p=0.1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d534362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W1 = nn.Linear(2*model_dim, 1, bias=False)\n",
    "        \n",
    "        self.W2 = nn.Linear(2*model_dim, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, M1, M2, M3, c_mask):\n",
    "        \n",
    "        start = torch.cat([M1,M2], dim=2)\n",
    "        \n",
    "        start = self.W1(start).squeeze()\n",
    "        \n",
    "        p1 = start.masked_fill(c_mask==1, -1e10)\n",
    "        \n",
    "        #p1 = F.log_softmax(start.masked_fill(c_mask==1, -1e10), dim=1)\n",
    "        \n",
    "        end = torch.cat([M1, M3], dim=2)\n",
    "        \n",
    "        end = self.W2(end).squeeze()\n",
    "        \n",
    "        p2 = end.masked_fill(c_mask==1, -1e10)\n",
    "        \n",
    "        #p2 = F.log_softmax(end.masked_fill(c_mask==1, -1e10), dim=1)\n",
    "        \n",
    "        #print(\"preds: \", [p1.shape,p2.shape])\n",
    "        return p1, p2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f3a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, word_emb_dim, kernel_size, model_dim, num_heads, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = EmbeddingLayer(char_vocab_dim, char_emb_dim, kernel_size, device)\n",
    "        \n",
    "        self.ctx_resizer = DepthwiseSeparableConvolution(char_emb_dim+word_emb_dim, model_dim, 5)\n",
    "        \n",
    "        self.qtn_resizer = DepthwiseSeparableConvolution(char_emb_dim+word_emb_dim, model_dim, 5)\n",
    "        \n",
    "        self.embedding_encoder = EncoderBlock(model_dim, num_heads, 4, 5, device)\n",
    "        \n",
    "        self.c2q_attention = ContextQueryAttentionLayer(model_dim)\n",
    "        \n",
    "        self.c2q_resizer = DepthwiseSeparableConvolution(model_dim*4, model_dim, 5)\n",
    "        \n",
    "        self.model_encoder_layers = nn.ModuleList([EncoderBlock(model_dim, num_heads, 2, 5, device)\n",
    "                                                   for _ in range(7)])\n",
    "        \n",
    "        self.output = OutputLayer(model_dim)\n",
    "        \n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self, ctx, qtn, ctx_char, qtn_char):\n",
    "        \n",
    "        c_mask = torch.eq(ctx, 1).float().to(self.device)\n",
    "        q_mask = torch.eq(qtn, 1).float().to(self.device)\n",
    "        \n",
    "        ctx_emb = self.embedding(ctx, ctx_char)\n",
    "        # [bs, ctx_len, ch_emb_dim + word_emb_dim]\n",
    "        \n",
    "        ctx_emb = self.ctx_resizer(ctx_emb)\n",
    "        #  [bs, ctx_len, model_dim]\n",
    "        \n",
    "        qtn_emb = self.embedding(qtn, qtn_char)\n",
    "        # [bs, ctx_len, ch_emb_dim + word_emb_dim]\n",
    "        \n",
    "        qtn_emb = self.qtn_resizer(qtn_emb)\n",
    "        # [bs, qtn_len, model_dim]\n",
    "        \n",
    "        C = self.embedding_encoder(ctx_emb, c_mask)\n",
    "        # [bs, ctx_len, model_dim]\n",
    "        \n",
    "        Q = self.embedding_encoder(qtn_emb, q_mask)\n",
    "        # [bs, qtn_len, model_dim]\n",
    "            \n",
    "        C2Q = self.c2q_attention(C, Q, c_mask, q_mask)\n",
    "        # [bs, ctx_len, model_dim*4]\n",
    "        \n",
    "        M1 = self.c2q_resizer(C2Q)\n",
    "        # [bs, ctx_len, model_dim]\n",
    "    \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M1 = layer(M1, c_mask)\n",
    "        \n",
    "        M2 = M1\n",
    "        # [bs, ctx_len, model_dim]  \n",
    "        \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M2 = layer(M2, c_mask)\n",
    "        \n",
    "        M3 = M2\n",
    "        # [bs, ctx_len, model_dim]\n",
    "        \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M3 = layer(M3, c_mask)\n",
    "            \n",
    "        p1, p2 = self.output(M1, M2, M3, c_mask)\n",
    "        \n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2365f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "CHAR_EMB_DIM = 200\n",
    "WORD_EMB_DIM = 300\n",
    "KERNEL_SIZE = 5\n",
    "MODEL_DIM = 128\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EncoderTransformer(CHAR_VOCAB_DIM,\n",
    "              CHAR_EMB_DIM, \n",
    "              WORD_EMB_DIM,\n",
    "              KERNEL_SIZE,\n",
    "              MODEL_DIM,\n",
    "              NUM_ATTENTION_HEADS,\n",
    "              device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5e9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,274,096 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2e2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), betas=(0.8,0.999), eps=10e-7, weight_decay=3*10e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d00c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    print(\"Starting training ........\")\n",
    "   \n",
    "\n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in train_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "        \n",
    "        # place data on GPU\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                    char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "        \n",
    "        # forward pass, get predictions\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        \n",
    "        # separate labels for start and end position\n",
    "        start_label, end_label = label[:,0], label[:,1]\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_label) + F.cross_entropy(end_pred, end_label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the gradients so that they do not accumulate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c576fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset):\n",
    "    \n",
    "    print(\"Starting testing .........\")\n",
    "   \n",
    "    test_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in test_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                    char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            y1, y2 = label[:,0], label[:,1]\n",
    "\n",
    "            loss = F.nll_loss(p1, y1) + F.nll_loss(p2, y2)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "    em, f1 = evaluate(predictions)\n",
    "    return test_loss/len(test_dataset), em, f1           \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cbcfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The testing dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open(test_dataset_path,'r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7871a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d7f466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.824336879916863| Time: 69m 53s\n",
      "Epoch test loss: -0.29787142138595873\n",
      "Epoch EM: 25.76158940397351\n",
      "Epoch F1: 36.812573908540756\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.49373696281212| Time: 68m 21s\n",
      "Epoch test loss: 0.6783412419346996\n",
      "Epoch EM: 26.887417218543046\n",
      "Epoch F1: 38.58954422235859\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.194981333569804| Time: 68m 3s\n",
      "Epoch test loss: -0.8007153533311657\n",
      "Epoch EM: 28.514664143803216\n",
      "Epoch F1: 39.85978732915494\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.9457253420315412| Time: 68m 25s\n",
      "Epoch test loss: -2.2653673490378217\n",
      "Epoch EM: 30.596026490066226\n",
      "Epoch F1: 41.7398517658064\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.73041032432649| Time: 68m 27s\n",
      "Epoch test loss: -0.4104823736360235\n",
      "Epoch EM: 31.44749290444655\n",
      "Epoch F1: 42.34626474956318\n",
      "====================================================================================\n",
      "Epoch 6\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.5504050019758098| Time: 68m 8s\n",
      "Epoch test loss: -1.163811306746353\n",
      "Epoch EM: 32.88552507095554\n",
      "Epoch F1: 43.68944468117555\n",
      "====================================================================================\n",
      "Epoch 7\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.3843751437927567| Time: 68m 53s\n",
      "Epoch test loss: -0.30053749001222035\n",
      "Epoch EM: 33.15988647114475\n",
      "Epoch F1: 44.20267629215945\n",
      "====================================================================================\n",
      "Epoch 8\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.2499630037578604| Time: 98m 54s\n",
      "Epoch test loss: -0.02623896633445377\n",
      "Epoch EM: 33.23557237464522\n",
      "Epoch F1: 44.518961280054604\n",
      "====================================================================================\n",
      "Epoch 9\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.1402211390012194| Time: 94m 21s\n",
      "Epoch test loss: -0.5950883245882643\n",
      "Epoch EM: 33.68968779564806\n",
      "Epoch F1: 45.10745962089553\n",
      "====================================================================================\n",
      "Epoch 10\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.0437900766335777| Time: 72m 22s\n",
      "Epoch test loss: -0.21059197757585507\n",
      "Epoch EM: 33.62346263008514\n",
      "Epoch F1: 44.86486144095311\n",
      "====================================================================================\n",
      "Epoch 11\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 2.9684740631442623| Time: 134m 27s\n",
      "Epoch test loss: 0.3585768653036143\n",
      "Epoch EM: 34.01135288552507\n",
      "Epoch F1: 44.97297872015192\n",
      "====================================================================================\n",
      "Epoch 12\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 2.8694684459498316| Time: 76m 17s\n",
      "Epoch test loss: 0.1703800396215468\n",
      "Epoch EM: 34.69252601702933\n",
      "Epoch F1: 46.008011281354726\n",
      "====================================================================================\n",
      "Epoch 13\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 2.812546027576321| Time: 84m 38s\n",
      "Epoch test loss: 0.7566263274600108\n",
      "Epoch EM: 33.55723746452223\n",
      "Epoch F1: 45.40310107260476\n",
      "====================================================================================\n",
      "Epoch 14\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 2.7507507048631417| Time: 82m 9s\n",
      "Epoch test loss: 0.6343521756482936\n",
      "Epoch EM: 33.10312204351939\n",
      "Epoch F1: 44.554843076735544\n",
      "====================================================================================\n",
      "Epoch 15\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m test_loss, em, f1 \u001b[38;5;241m=\u001b[39m test(model, test_dataset)\n\u001b[0;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset)\u001b[0m\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# update the gradients\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# zero the gradients so that they do not accumulate\u001b[39;00m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py39\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py39\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py39\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\.conda\\envs\\py39\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py39\\lib\\site-packages\\torch\\optim\\adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    262\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 263\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[0;32m    266\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    test_loss, em, f1 = test(model, test_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch test loss: {test_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "CHAR_EMB_DIM = 200\n",
    "WORD_EMB_DIM = 300\n",
    "KERNEL_SIZE = 5\n",
    "MODEL_DIM = 128\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EncoderTransformer(CHAR_VOCAB_DIM,\n",
    "              CHAR_EMB_DIM, \n",
    "              WORD_EMB_DIM,\n",
    "              KERNEL_SIZE,\n",
    "              MODEL_DIM,\n",
    "              NUM_ATTENTION_HEADS,\n",
    "              device).to(device)\n",
    "\n",
    "model_copy.load_state_dict(torch.load(model_path))\n",
    "model_copy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ee270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
