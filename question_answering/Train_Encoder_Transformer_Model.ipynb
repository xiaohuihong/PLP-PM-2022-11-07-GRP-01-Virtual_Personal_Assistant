{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4a3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from preprocess import *\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = './dataset/squad_test.json'\n",
    "model_path = './model/model_encoder_transformer.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4124db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed data from pickle\n",
    "train_df = pd.read_pickle('./dataset/qatrain.pkl')\n",
    "test_df = pd.read_pickle('./dataset/qatest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c955a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.load('./dataset/qaglove_vt.npy')\n",
    "word2idx = np.load('./dataset/qa_word2idx.npy', allow_pickle=True).item()\n",
    "idx2word = np.load('./dataset/qa_idx2word.npy', allow_pickle=True).item()\n",
    "char2idx = np.load('./dataset/qa_char2idx.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8b0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    - Creates batches dynamically by padding to the length of largest example\n",
    "      in a given batch.\n",
    "    - Calulates character vectors for contexts and question.\n",
    "    - Returns tensors for training.\n",
    "    '''\n",
    "    def __init__(self, data, batch_size):\n",
    "        '''\n",
    "        data: dataframe\n",
    "        batch_size: int\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, sentence, max_word_len=16):\n",
    "        \n",
    "        char_vec = torch.zeros(max_sent_len, max_word_len).type(torch.LongTensor)\n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                if j == max_word_len:\n",
    "                    break\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)\n",
    "        \n",
    "        return char_vec     \n",
    "    \n",
    "    def get_span(self, text):\n",
    "\n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :char_ctx & ques_ctx: character-level ids for context and question\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while testing to calculate metrics\n",
    "        :ids: question_ids for evaluation\n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "            \n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "             \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "                \n",
    "            max_word_ctx = 16\n",
    "          \n",
    "            char_ctx = torch.zeros(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, context)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 16\n",
    "            \n",
    "            char_ques = torch.zeros(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, question)\n",
    "            \n",
    "              \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            ids = list(batch.id)\n",
    "            \n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee6d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataset = SquadDataset(train_df,16)\n",
    "test_dataset = SquadDataset(test_df,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79eebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if dim == 2:\n",
    "            \n",
    "            self.depthwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels,\n",
    "                                        kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2)\n",
    "        \n",
    "            self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    \n",
    "        else:\n",
    "        \n",
    "            self.depthwise_conv = nn.Conv1d(in_channels=in_channels, out_channels=in_channels,\n",
    "                                            kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2,\n",
    "                                            bias=False)\n",
    "\n",
    "            self.pointwise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, emb_dim]\n",
    "        if self.dim == 1:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.pointwise_conv(self.depthwise_conv(x))\n",
    "            x = x.transpose(1,2)\n",
    "        else:\n",
    "            x = self.pointwise_conv(self.depthwise_conv(x))\n",
    "        #print(\"DepthWiseConv output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b589e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_dim, num_layers=2):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layers = nn.ModuleList([nn.Linear(layer_dim, layer_dim) for _ in range(num_layers)])\n",
    "        self.gate_layers = nn.ModuleList([nn.Linear(layer_dim, layer_dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"Highway input: \", x.shape)\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow = self.flow_layers[i](x)\n",
    "            gate = torch.sigmoid(self.gate_layers[i](x))\n",
    "            \n",
    "            x = gate * flow + (1 - gate) * x\n",
    "            \n",
    "        #print(\"Highway output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e245e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, kernel_size, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim)\n",
    "        \n",
    "        self.word_embedding = self.get_glove_word_embedding()\n",
    "        \n",
    "        self.conv2d = DepthwiseSeparableConvolution(char_emb_dim, char_emb_dim, kernel_size,dim=2)\n",
    "        \n",
    "        self.highway = HighwayLayer(self.word_emb_dim + char_emb_dim)\n",
    "    \n",
    "        \n",
    "    def get_glove_word_embedding(self):\n",
    "        \n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.word_emb_dim = embedding_dim\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x, x_char):\n",
    "        # x = [bs, seq_len]\n",
    "        # x_char = [bs, seq_len, word_len(=16)]\n",
    "        \n",
    "        word_emb = self.word_embedding(x)\n",
    "        # word_emb = [bs, seq_len, word_emb_dim]\n",
    "        \n",
    "        word_emb = F.dropout(word_emb,p=0.1)\n",
    "        \n",
    "        char_emb = self.char_embedding(x_char)\n",
    "        # char_embed = [bs, seq_len, word_len, char_emb_dim]\n",
    "        \n",
    "        char_emb = F.dropout(char_emb.permute(0,3,1,2), p=0.05)\n",
    "        # [bs, char_emb_dim, seq_len, word_len] == [N, Cin, Hin, Win]\n",
    "        \n",
    "        conv_out = F.relu(self.conv2d(char_emb))\n",
    "        # [bs, char_emb_dim, seq_len, word_len] \n",
    "        # the depthwise separable conv does not change the shape of the input\n",
    "        \n",
    "        char_emb, _ = torch.max(conv_out, dim=3)\n",
    "        # [bs, char_emb_dim, seq_len]\n",
    "        \n",
    "        char_emb = char_emb.permute(0,2,1)\n",
    "        # [bs, seq_len, char_emb_dim]\n",
    "        \n",
    "        concat_emb = torch.cat([char_emb, word_emb], dim=2)\n",
    "        # [bs, seq_len, char_emb_dim + word_emb_dim]\n",
    "        \n",
    "        emb = self.highway(concat_emb)\n",
    "        # [bs, seq_len, char_emb_dim + word_emb_dim]\n",
    "        \n",
    "        #print(\"Embedding output: \", emb.shape)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad294877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, num_heads, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.head_dim = self.hid_dim // self.num_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x = [bs, len_x, hid_dim]\n",
    "        # mask = [bs, len_x]\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(x)\n",
    "        K = self.fc_k(x)\n",
    "        V = self.fc_v(x)\n",
    "        # Q = K = V = [bs, len_x, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        # [bs, len_x, num_heads, head_dim ]  => [bs, num_heads, len_x, head_dim]\n",
    "        \n",
    "        K = K.permute(0,1,3,2)\n",
    "        # [bs, num_heads, head_dim, len_x]\n",
    "        \n",
    "        energy = torch.matmul(Q, K) / self.scale\n",
    "        # (bs, num_heads){[len_x, head_dim] * [head_dim, len_x]} => [bs, num_heads, len_x, len_x]\n",
    "        \n",
    "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        # [bs, 1, 1, len_x]\n",
    "        \n",
    "        #print(\"Mask: \", mask)\n",
    "        #print(\"Energy: \", energy)\n",
    "        \n",
    "        energy = energy.masked_fill(mask == 1, -1e10)\n",
    "        \n",
    "        #print(\"energy after masking: \", energy)\n",
    "        \n",
    "        alpha = torch.softmax(energy, dim=-1)\n",
    "        #  [bs, num_heads, len_x, len_x]\n",
    "        \n",
    "        #print(\"energy after smax: \", alpha)\n",
    "        alpha = F.dropout(alpha, p=0.1)\n",
    "        \n",
    "        a = torch.matmul(alpha, V)\n",
    "        # [bs, num_heads, len_x, head_dim]\n",
    "        \n",
    "        a = a.permute(0,2,1,3)\n",
    "        # [bs, len_x, num_heads, hid_dim]\n",
    "        \n",
    "        a = a.contiguous().view(batch_size, -1, self.hid_dim)\n",
    "        # [bs, len_x, hid_dim]\n",
    "        \n",
    "        a = self.fc_o(a)\n",
    "        # [bs, len_x, hid_dim]\n",
    "        \n",
    "        #print(\"Multihead output: \", a.shape)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dda8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, device, max_length=400):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_length, model_dim)\n",
    "        \n",
    "        for pos in range(max_length):\n",
    "            \n",
    "            for i in range(0, model_dim, 2):\n",
    "                \n",
    "                pos_encoding[pos, i] = math.sin(pos / (10000 ** ((2*i)/model_dim)))\n",
    "                pos_encoding[pos, i+1] = math.cos(pos / (10000 ** ((2*(i+1))/model_dim)))\n",
    "            \n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).to(device)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"PE shape: \", self.pos_encoding.shape)\n",
    "        #print(\"PE input: \", x.shape)\n",
    "        x = x + Variable(self.pos_encoding[:, :x.shape[1]], requires_grad=False)\n",
    "        #print(\"PE output: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4063734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, num_heads, num_conv_layers, kernel_size, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList([DepthwiseSeparableConvolution(model_dim, model_dim, kernel_size)\n",
    "                                          for _ in range(num_conv_layers)])\n",
    "        \n",
    "        self.multihead_self_attn = MultiheadAttentionLayer(model_dim, num_heads, device)\n",
    "        \n",
    "        self.position_encoder = PositionEncoder(model_dim, device)\n",
    "        \n",
    "        self.pos_norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.conv_norm = nn.ModuleList([nn.LayerNorm(model_dim) for _ in range(self.num_conv_layers)])\n",
    "        \n",
    "        self.feedfwd_norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.feed_fwd = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # x = [bs, len_x, model_dim]\n",
    "        # mask = [bs, len_x]\n",
    "        \n",
    "        out = self.position_encoder(x)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        res = out\n",
    "        \n",
    "        out = self.pos_norm(out)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            \n",
    "            out = F.relu(conv_layer(out))\n",
    "            out = out + res\n",
    "            if (i+1) % 2 == 0:\n",
    "                out = F.dropout(out, p=0.1)\n",
    "            res = out\n",
    "            out = self.conv_norm[i](out)\n",
    "        \n",
    "        \n",
    "        out = self.multihead_self_attn(out, mask)\n",
    "        # [bs, len_x, model_dim]\n",
    "        \n",
    "        out = F.dropout(out + res, p=0.1)\n",
    "        \n",
    "        res = out\n",
    "        \n",
    "        out = self.feedfwd_norm(out)\n",
    "        \n",
    "        out = F.relu(self.feed_fwd(out))\n",
    "        # [bs, len_x, model_dim]\n",
    "            \n",
    "        out = F.dropout(out + res, p=0.1)\n",
    "        # [bs, len_x, model_dim]\n",
    "        #print(\"Encoder block output: \", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d1ed6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQueryAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.W0 = nn.Linear(3*model_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, C, Q, c_mask, q_mask):\n",
    "        # C = [bs, ctx_len, model_dim]\n",
    "        # Q = [bs, qtn_len, model_dim]\n",
    "        # c_mask = [bs, ctx_len]\n",
    "        # q_mask = [bs, qtn_len]\n",
    "        \n",
    "        c_mask = c_mask.unsqueeze(2)\n",
    "        # [bs, ctx_len, 1]\n",
    "        \n",
    "        q_mask = q_mask.unsqueeze(1)\n",
    "        # [bs, 1, qtn_len]\n",
    "        \n",
    "        ctx_len = C.shape[1]\n",
    "        qtn_len = Q.shape[1]\n",
    "        \n",
    "        C_ = C.unsqueeze(2).repeat(1,1,qtn_len,1)\n",
    "        # [bs, ctx_len, qtn_len, model_dim] \n",
    "        \n",
    "        Q_ = Q.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        # [bs, ctx_len, qtn_len, model_dim]\n",
    "        \n",
    "        C_elemwise_Q = torch.mul(C_, Q_)\n",
    "        # [bs, ctx_len, qtn_len, model_dim]\n",
    "        \n",
    "        S = torch.cat([C_, Q_, C_elemwise_Q], dim=3)\n",
    "        # [bs, ctx_len, qtn_len, model_dim*3]\n",
    "        \n",
    "        S = self.W0(S).squeeze()\n",
    "        #print(\"Simi matrix: \", S.shape)\n",
    "        # [bs, ctx_len, qtn_len, 1] => # [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        S_row = S.masked_fill(q_mask==1, -1e10)\n",
    "        S_row = F.softmax(S_row, dim=2)\n",
    "        \n",
    "        S_col = S.masked_fill(c_mask==1, -1e10)\n",
    "        S_col = F.softmax(S_col, dim=1)\n",
    "        \n",
    "        A = torch.bmm(S_row, Q)\n",
    "        # (bs)[ctx_len, qtn_len] X [qtn_len, model_dim] => [bs, ctx_len, model_dim]\n",
    "        \n",
    "        B = torch.bmm(torch.bmm(S_row,S_col.transpose(1,2)), C)\n",
    "        # [ctx_len, qtn_len] X [qtn_len, ctx_len] => [bs, ctx_len, ctx_len]\n",
    "        # [ctx_len, ctx_len] X [ctx_len, model_dim ] => [bs, ctx_len, model_dim]\n",
    "        \n",
    "        model_out = torch.cat([C, A, torch.mul(C,A), torch.mul(C,B)], dim=2)\n",
    "        # [bs, ctx_len, model_dim*4]\n",
    "        \n",
    "        #print(\"C2Q output: \", model_out.shape)\n",
    "        return F.dropout(model_out, p=0.1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d534362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W1 = nn.Linear(2*model_dim, 1, bias=False)\n",
    "        \n",
    "        self.W2 = nn.Linear(2*model_dim, 1, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, M1, M2, M3, c_mask):\n",
    "        \n",
    "        start = torch.cat([M1,M2], dim=2)\n",
    "        \n",
    "        start = self.W1(start).squeeze()\n",
    "        \n",
    "        p1 = start.masked_fill(c_mask==1, -1e10)\n",
    "        \n",
    "        #p1 = F.log_softmax(start.masked_fill(c_mask==1, -1e10), dim=1)\n",
    "        \n",
    "        end = torch.cat([M1, M3], dim=2)\n",
    "        \n",
    "        end = self.W2(end).squeeze()\n",
    "        \n",
    "        p2 = end.masked_fill(c_mask==1, -1e10)\n",
    "        \n",
    "        #p2 = F.log_softmax(end.masked_fill(c_mask==1, -1e10), dim=1)\n",
    "        \n",
    "        #print(\"preds: \", [p1.shape,p2.shape])\n",
    "        return p1, p2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f3a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, word_emb_dim, kernel_size, model_dim, num_heads, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = EmbeddingLayer(char_vocab_dim, char_emb_dim, kernel_size, device)\n",
    "        \n",
    "        self.ctx_resizer = DepthwiseSeparableConvolution(char_emb_dim+word_emb_dim, model_dim, 5)\n",
    "        \n",
    "        self.qtn_resizer = DepthwiseSeparableConvolution(char_emb_dim+word_emb_dim, model_dim, 5)\n",
    "        \n",
    "        self.embedding_encoder = EncoderBlock(model_dim, num_heads, 4, 5, device)\n",
    "        \n",
    "        self.c2q_attention = ContextQueryAttentionLayer(model_dim)\n",
    "        \n",
    "        self.c2q_resizer = DepthwiseSeparableConvolution(model_dim*4, model_dim, 5)\n",
    "        \n",
    "        self.model_encoder_layers = nn.ModuleList([EncoderBlock(model_dim, num_heads, 2, 5, device)\n",
    "                                                   for _ in range(7)])\n",
    "        \n",
    "        self.output = OutputLayer(model_dim)\n",
    "        \n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self, ctx, qtn, ctx_char, qtn_char):\n",
    "        \n",
    "        c_mask = torch.eq(ctx, 1).float().to(self.device)\n",
    "        q_mask = torch.eq(qtn, 1).float().to(self.device)\n",
    "        \n",
    "        ctx_emb = self.embedding(ctx, ctx_char)\n",
    "        # [bs, ctx_len, ch_emb_dim + word_emb_dim]\n",
    "        \n",
    "        ctx_emb = self.ctx_resizer(ctx_emb)\n",
    "        #  [bs, ctx_len, model_dim]\n",
    "        \n",
    "        qtn_emb = self.embedding(qtn, qtn_char)\n",
    "        # [bs, ctx_len, ch_emb_dim + word_emb_dim]\n",
    "        \n",
    "        qtn_emb = self.qtn_resizer(qtn_emb)\n",
    "        # [bs, qtn_len, model_dim]\n",
    "        \n",
    "        C = self.embedding_encoder(ctx_emb, c_mask)\n",
    "        # [bs, ctx_len, model_dim]\n",
    "        \n",
    "        Q = self.embedding_encoder(qtn_emb, q_mask)\n",
    "        # [bs, qtn_len, model_dim]\n",
    "            \n",
    "        C2Q = self.c2q_attention(C, Q, c_mask, q_mask)\n",
    "        # [bs, ctx_len, model_dim*4]\n",
    "        \n",
    "        M1 = self.c2q_resizer(C2Q)\n",
    "        # [bs, ctx_len, model_dim]\n",
    "    \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M1 = layer(M1, c_mask)\n",
    "        \n",
    "        M2 = M1\n",
    "        # [bs, ctx_len, model_dim]  \n",
    "        \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M2 = layer(M2, c_mask)\n",
    "        \n",
    "        M3 = M2\n",
    "        # [bs, ctx_len, model_dim]\n",
    "        \n",
    "        for layer in self.model_encoder_layers:\n",
    "            M3 = layer(M3, c_mask)\n",
    "            \n",
    "        p1, p2 = self.output(M1, M2, M3, c_mask)\n",
    "        \n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2365f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "CHAR_EMB_DIM = 200\n",
    "WORD_EMB_DIM = 300\n",
    "KERNEL_SIZE = 5\n",
    "MODEL_DIM = 128\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EncoderTransformer(CHAR_VOCAB_DIM,\n",
    "              CHAR_EMB_DIM, \n",
    "              WORD_EMB_DIM,\n",
    "              KERNEL_SIZE,\n",
    "              MODEL_DIM,\n",
    "              NUM_ATTENTION_HEADS,\n",
    "              device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5e9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,274,096 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2e2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), betas=(0.8,0.999), eps=10e-7, weight_decay=3*10e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d00c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    print(\"Starting training ........\")\n",
    "   \n",
    "\n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in train_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "        \n",
    "        # place data on GPU\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                    char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "        \n",
    "        # forward pass, get predictions\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        \n",
    "        # separate labels for start and end position\n",
    "        start_label, end_label = label[:,0], label[:,1]\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_label) + F.cross_entropy(end_pred, end_label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the gradients so that they do not accumulate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c576fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset):\n",
    "    \n",
    "    print(\"Starting testing .........\")\n",
    "   \n",
    "    test_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in test_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                    char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            y1, y2 = label[:,0], label[:,1]\n",
    "\n",
    "            loss = F.nll_loss(p1, y1) + F.nll_loss(p2, y2)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "    em, f1 = evaluate(predictions)\n",
    "    return test_loss/len(test_dataset), em, f1           \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cbcfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The testing dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open(test_dataset_path,'r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7871a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d7f466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 8.836569548448516| Time: 48m 48s\n",
      "Epoch test loss: 1.4434162497814826\n",
      "Epoch EM: 11.30558183538316\n",
      "Epoch F1: 20.297960090812612\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 6.1158188681765076| Time: 42m 28s\n",
      "Epoch test loss: -1.1477110910133888\n",
      "Epoch EM: 17.105014191106907\n",
      "Epoch F1: 27.169447316803655\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 5.477163438690071| Time: 42m 19s\n",
      "Epoch test loss: -0.9858868062173771\n",
      "Epoch EM: 22.68684957426679\n",
      "Epoch F1: 32.94996781921284\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 5.003947288990904| Time: 42m 13s\n",
      "Epoch test loss: -0.913010479367197\n",
      "Epoch EM: 24.22894985808893\n",
      "Epoch F1: 35.0474018217787\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.62658757470642| Time: 42m 2s\n",
      "Epoch test loss: -1.1561960077153297\n",
      "Epoch EM: 27.26584673604541\n",
      "Epoch F1: 38.40945034477073\n",
      "====================================================================================\n",
      "Epoch 6\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.288351106502287| Time: 42m 6s\n",
      "Epoch test loss: -0.023298821586376077\n",
      "Epoch EM: 27.483443708609272\n",
      "Epoch F1: 38.62339310053228\n",
      "====================================================================================\n",
      "Epoch 7\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.005906087632883| Time: 51m 54s\n",
      "Epoch test loss: -0.4028958334225674\n",
      "Epoch EM: 29.35666982024598\n",
      "Epoch F1: 40.793756106087436\n",
      "====================================================================================\n",
      "Epoch 8\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.7647723050759048| Time: 44m 46s\n",
      "Epoch test loss: -0.7045684613494215\n",
      "Epoch EM: 30.99337748344371\n",
      "Epoch F1: 41.90609361028485\n",
      "====================================================================================\n",
      "Epoch 9\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.5856761238016492| Time: 43m 48s\n",
      "Epoch test loss: 0.060963730801769506\n",
      "Epoch EM: 32.459791863765375\n",
      "Epoch F1: 43.26826796043581\n",
      "====================================================================================\n",
      "Epoch 10\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.3878967760667877| Time: 45m 32s\n",
      "Epoch test loss: 0.5835446247380658\n",
      "Epoch EM: 34.10596026490066\n",
      "Epoch F1: 44.96633594930618\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    test_loss, em, f1 = test(model, test_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch test loss: {test_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dcf6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559b2e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderTransformer(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (char_embedding): Embedding(232, 200)\n",
       "    (word_embedding): Embedding(110474, 300)\n",
       "    (conv2d): DepthwiseSeparableConvolution(\n",
       "      (depthwise_conv): Conv2d(200, 200, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=200)\n",
       "      (pointwise_conv): Conv2d(200, 200, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (highway): HighwayLayer(\n",
       "      (flow_layers): ModuleList(\n",
       "        (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "        (1): Linear(in_features=500, out_features=500, bias=True)\n",
       "      )\n",
       "      (gate_layers): ModuleList(\n",
       "        (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "        (1): Linear(in_features=500, out_features=500, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ctx_resizer): DepthwiseSeparableConvolution(\n",
       "    (depthwise_conv): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), groups=500, bias=False)\n",
       "    (pointwise_conv): Conv1d(500, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (qtn_resizer): DepthwiseSeparableConvolution(\n",
       "    (depthwise_conv): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), groups=500, bias=False)\n",
       "    (pointwise_conv): Conv1d(500, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (embedding_encoder): EncoderBlock(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): DepthwiseSeparableConvolution(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): DepthwiseSeparableConvolution(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): DepthwiseSeparableConvolution(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): DepthwiseSeparableConvolution(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (multihead_self_attn): MultiheadAttentionLayer(\n",
       "      (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (position_encoder): PositionEncoder()\n",
       "    (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv_norm): ModuleList(\n",
       "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (c2q_attention): ContextQueryAttentionLayer(\n",
       "    (W0): Linear(in_features=384, out_features=1, bias=False)\n",
       "  )\n",
       "  (c2q_resizer): DepthwiseSeparableConvolution(\n",
       "    (depthwise_conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), groups=512, bias=False)\n",
       "    (pointwise_conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (model_encoder_layers): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (3): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (4): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (5): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (6): EncoderBlock(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConvolution(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128, bias=False)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (multihead_self_attn): MultiheadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_encoder): PositionEncoder()\n",
       "      (pos_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_norm): ModuleList(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feedfwd_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_fwd): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output): OutputLayer(\n",
       "    (W1): Linear(in_features=256, out_features=1, bias=False)\n",
       "    (W2): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "CHAR_EMB_DIM = 200\n",
    "WORD_EMB_DIM = 300\n",
    "KERNEL_SIZE = 5\n",
    "MODEL_DIM = 128\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = EncoderTransformer(CHAR_VOCAB_DIM,\n",
    "              CHAR_EMB_DIM, \n",
    "              WORD_EMB_DIM,\n",
    "              KERNEL_SIZE,\n",
    "              MODEL_DIM,\n",
    "              NUM_ATTENTION_HEADS,\n",
    "              device).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ee270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.7028304195518933| Time: 79m 57s\n",
      "Epoch test loss: 0.6183365958577209\n",
      "Epoch EM: 34.730368968779565\n",
      "Epoch F1: 45.60177003847775\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.7152117548097583| Time: 54m 13s\n",
      "Epoch test loss: 0.5812381080431498\n",
      "Epoch EM: 34.15326395458846\n",
      "Epoch F1: 45.21826907529421\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 3.710224326503904| Time: 43m 14s\n",
      "Epoch test loss: 0.6033278486795675\n",
      "Epoch EM: 34.38978240302744\n",
      "Epoch F1: 45.595293707811976\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting batch: 3000\n",
      "Starting batch: 3500\n",
      "Starting batch: 4000\n",
      "Starting batch: 4500\n",
      "Starting batch: 5000\n",
      "Starting testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    test_loss, em, f1 = test(model, test_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch test loss: {test_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec3c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
