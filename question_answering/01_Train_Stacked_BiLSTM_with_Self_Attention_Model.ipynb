{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea9ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438bacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed data from pickle\n",
    "train_df = pd.read_pickle('./dataset/qatrain.pkl')\n",
    "test_df = pd.read_pickle('./dataset/qatest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0687bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.load('./dataset/qaglove_vt.npy')\n",
    "word2idx = np.load('./dataset/qa_word2idx.npy', allow_pickle=True).item()\n",
    "idx2word = np.load('./dataset/qa_idx2word.npy', allow_pickle=True).item()\n",
    "char2idx = np.load('./dataset/qa_char2idx.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0caeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    -Divides the dataframe in batches.\n",
    "    -Pads the contexts and questions dynamically for each batch by padding \n",
    "     the examples to the maximum-length sequence in that batch.\n",
    "    -Calculates masks for context and question.\n",
    "    -Calculates spans for contexts.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :context_mask & question_mask: zero-mask for question and context\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while testing to calculate metrics\n",
    "        :context_spans: spans of context text\n",
    "        :ids: question_ids used in evaluation\n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "                            \n",
    "            spans = []\n",
    "            context_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                context_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i,: len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            context_mask = torch.eq(padded_context, 1)\n",
    "            question_mask = torch.eq(padded_question, 1)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            \n",
    "            yield (padded_context, padded_question, context_mask, \n",
    "                   question_mask, label, context_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "443a8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_df, 32)\n",
    "test_dataset = SquadDataset(test_df, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ea2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignQuestionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, context, question, question_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, emb_dim]\n",
    "        # question = [bs, qtn_len, emb_dim]\n",
    "        # question_mask = [bs, qtn_len]\n",
    "    \n",
    "        ctx_ = self.linear(context)\n",
    "        ctx_ = self.relu(ctx_)\n",
    "        # ctx_ = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        qtn_ = self.linear(question)\n",
    "        qtn_ = self.relu(qtn_)\n",
    "        # qtn_ = [bs, qtn_len, emb_dim]\n",
    "        \n",
    "        qtn_transpose = qtn_.permute(0,2,1)\n",
    "        # qtn_transpose = [bs, emb_dim, qtn_len]\n",
    "        \n",
    "        align_scores = torch.bmm(ctx_, qtn_transpose)\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        qtn_mask = question_mask.unsqueeze(1).expand(align_scores.size())\n",
    "        # qtn_mask = [bs, 1, qtn_len] => [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        # Fills elements of self tensor(align_scores) with value(-float(inf)) where mask is True. \n",
    "        # The shape of mask must be broadcastable with the shape of the underlying tensor.\n",
    "        align_scores = align_scores.masked_fill(qtn_mask == 1, -float('inf'))\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_scores_flat = align_scores.view(-1, question.size(1))\n",
    "        # align_scores = [bs*ctx_len, qtn_len]\n",
    "        \n",
    "        alpha = F.softmax(align_scores_flat, dim=1)\n",
    "        alpha = alpha.view(-1, context.shape[1], question.shape[1])\n",
    "        # alpha = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_embedding = torch.bmm(alpha, question)\n",
    "        # align = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        return align_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a962a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedBiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            \n",
    "            self.lstms.append(nn.LSTM(input_dim, hidden_dim,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "           \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, feature_dim]\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            lstm_input = outputs[-1]\n",
    "            lstm_out = F.dropout(lstm_input, p=self.dropout)\n",
    "            lstm_out, (hidden, cell) = self.lstms[i](lstm_input)\n",
    "           \n",
    "            outputs.append(lstm_out)\n",
    "\n",
    "    \n",
    "        output = torch.cat(outputs[1:], dim=2)\n",
    "        # [bs, seq_len, num_layers*num_dir*hidden_dim]\n",
    "        \n",
    "        output = F.dropout(output, p=self.dropout)\n",
    "      \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf62816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, question, question_mask):\n",
    "        \n",
    "        # question = [bs, qtn_len, input_dim] = [bs, qtn_len, bi_lstm_hid_dim]\n",
    "        # question_mask = [bs,  qtn_len]\n",
    "        \n",
    "        qtn = question.view(-1, question.shape[-1])\n",
    "        # qtn = [bs*qtn_len, hid_dim]\n",
    "        \n",
    "        attn_scores = self.linear(qtn)\n",
    "        # attn_scores = [bs*qtn_len, 1]\n",
    "        \n",
    "        attn_scores = attn_scores.view(question.shape[0], question.shape[1])\n",
    "        # attn_scores = [bs, qtn_len]\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(question_mask == 1, -float('inf'))\n",
    "        \n",
    "        alpha = F.softmax(attn_scores, dim=1)\n",
    "        # alpha = [bs, qtn_len]\n",
    "        \n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d0d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(x, weights):\n",
    "    # x = [bs, len, dim]\n",
    "    # weights = [bs, len]\n",
    "    \n",
    "    weights = weights.unsqueeze(1)\n",
    "    # weights = [bs, 1, len]\n",
    "    \n",
    "    w = weights.bmm(x).squeeze(1)\n",
    "    # w = [bs, 1, dim] => [bs, dim]\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2e0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_dim, question_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(question_dim, context_dim)\n",
    "        \n",
    "    def forward(self, context, question, context_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, ctx_hid_dim] = [bs, ctx_len, hid_dim*6] = [bs, ctx_len, 768]\n",
    "        # question = [bs, qtn_hid_dim] = [bs, qtn_len, 768]\n",
    "        # context_mask = [bs, ctx_len]\n",
    "        \n",
    "        qtn_proj = self.linear(question)\n",
    "        # qtn_proj = [bs, ctx_hid_dim]\n",
    "        \n",
    "        qtn_proj = qtn_proj.unsqueeze(2)\n",
    "        # qtn_proj = [bs, ctx_hid_dim, 1]\n",
    "        \n",
    "        scores = context.bmm(qtn_proj)\n",
    "        # scores = [bs, ctx_len, 1]\n",
    "        \n",
    "        scores = scores.squeeze(2)\n",
    "        # scores = [bs, ctx_len]\n",
    "        \n",
    "        scores = scores.masked_fill(context_mask == 1, -float('inf'))\n",
    "        \n",
    "        #alpha = F.log_softmax(scores, dim=1)\n",
    "        # alpha = [bs, ctx_len]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab2ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentReader(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, num_directions, dropout, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #self.embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.context_bilstm = StackedBiLSTM(embedding_dim * 2, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.question_bilstm = StackedBiLSTM(embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.glove_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        def tune_embedding(grad, words=1000):\n",
    "            grad[words:] = 0\n",
    "            return grad\n",
    "        \n",
    "        self.glove_embedding.weight.register_hook(tune_embedding)\n",
    "        \n",
    "        self.align_embedding = AlignQuestionEmbedding(embedding_dim)\n",
    "        \n",
    "        self.linear_attn_question = LinearAttentionLayer(hidden_dim*num_layers*num_directions) \n",
    "        \n",
    "        self.bilinear_attn_start = BilinearAttentionLayer(hidden_dim*num_layers*num_directions, \n",
    "                                                          hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.bilinear_attn_end = BilinearAttentionLayer(hidden_dim*num_layers*num_directions,\n",
    "                                                        hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "        \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=False)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def forward(self, context, question, context_mask, question_mask):\n",
    "       \n",
    "        # context = [bs, len_c]\n",
    "        # question = [bs, len_q]\n",
    "        # context_mask = [bs, len_c]\n",
    "        # question_mask = [bs, len_q]\n",
    "        \n",
    "        \n",
    "        ctx_embed = self.glove_embedding(context)\n",
    "        # ctx_embed = [bs, len_c, emb_dim]\n",
    "        \n",
    "        ques_embed = self.glove_embedding(question)\n",
    "        # ques_embed = [bs, len_q, emb_dim]\n",
    "        \n",
    "\n",
    "        ctx_embed = self.dropout(ctx_embed)\n",
    "     \n",
    "        ques_embed = self.dropout(ques_embed)\n",
    "             \n",
    "        align_embed = self.align_embedding(ctx_embed, ques_embed, question_mask)\n",
    "        # align_embed = [bs, len_c, emb_dim]  \n",
    "        \n",
    "        ctx_bilstm_input = torch.cat([ctx_embed, align_embed], dim=2)\n",
    "        # ctx_bilstm_input = [bs, len_c, emb_dim*2]\n",
    "                \n",
    "        ctx_outputs = self.context_bilstm(ctx_bilstm_input)\n",
    "        # ctx_outputs = [bs, len_c, hid_dim*layers*dir] = [bs, len_c, hid_dim*6]\n",
    "       \n",
    "        qtn_outputs = self.question_bilstm(ques_embed)\n",
    "        # qtn_outputs = [bs, len_q, hid_dim*6]\n",
    "    \n",
    "        qtn_weights = self.linear_attn_question(qtn_outputs, question_mask)\n",
    "        # qtn_weights = [bs, len_q]\n",
    "            \n",
    "        qtn_weighted = weighted_average(qtn_outputs, qtn_weights)\n",
    "        # qtn_weighted = [bs, hid_dim*6]\n",
    "        \n",
    "        start_scores = self.bilinear_attn_start(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # start_scores = [bs, len_c]\n",
    "         \n",
    "        end_scores = self.bilinear_attn_end(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # end_scores = [bs, len_c]\n",
    "        \n",
    "      \n",
    "        return start_scores, end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e977948",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "HIDDEN_DIM = 128\n",
    "EMB_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_DIRECTIONS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = DocumentReader(HIDDEN_DIM,\n",
    "                       EMB_DIM, \n",
    "                       NUM_LAYERS, \n",
    "                       NUM_DIRECTIONS, \n",
    "                       DROPOUT, \n",
    "                       device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c07364",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ae9b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,183,349 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    '''Returns the number of trainable parameters in the model.'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee5fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting training ........\")\n",
    "    \n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    \n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through training data\n",
    "    for batch in train_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, ctx, ans, ids = batch\n",
    "        \n",
    "        # place the tensors on GPU\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "        \n",
    "        # forward pass, get the predictions\n",
    "        preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        \n",
    "        # separate labels for start and end position\n",
    "        start_label, end_label = label[:,0], label[:,1]\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_label) + F.cross_entropy(end_pred, end_label)\n",
    "        \n",
    "        # backward pass, calculates the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        \n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero the gradients to prevent them from accumulating\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7587193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset):\n",
    "    '''\n",
    "    Performs Testing.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting Testing .........\")\n",
    "   \n",
    "    test_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    # puts the model in eval mode. Turns off dropout\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in test_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, context_text, answers, ids = batch\n",
    "\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            y1, y2 = label[:,0], label[:,1]\n",
    "\n",
    "            loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            \n",
    "            # get the start and end index positions from the model preds\n",
    "            \n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            \n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "            # stack predictions\n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "            \n",
    "            \n",
    "    em, f1 = evaluate(predictions)            \n",
    "    return test_loss/len(test_dataset), em, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d24c5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The test dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('./dataset/squad_test.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee68f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcade8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 4.906239013348446| Time: 5m 40s\n",
      "Epoch test loss: 3.901615047611836\n",
      "Epoch EM: 47.59697256385998\n",
      "Epoch F1: 59.798274195910516\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 3.615682955447438| Time: 5m 45s\n",
      "Epoch test loss: 3.6368378899844322\n",
      "Epoch EM: 51.63670766319773\n",
      "Epoch F1: 63.51203302494549\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 3.1947938867239176| Time: 6m 0s\n",
      "Epoch test loss: 3.455752439635717\n",
      "Epoch EM: 54.87228003784295\n",
      "Epoch F1: 66.37819485911874\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.8998804734626287| Time: 5m 56s\n",
      "Epoch test loss: 3.3003336210241883\n",
      "Epoch EM: 56.72658467360454\n",
      "Epoch F1: 68.29851157286714\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.6828549475781207| Time: 5m 53s\n",
      "Epoch test loss: 3.307739968105856\n",
      "Epoch EM: 56.915799432355726\n",
      "Epoch F1: 68.60897616240804\n",
      "====================================================================================\n",
      "Epoch 6\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.4962882876970576| Time: 5m 54s\n",
      "Epoch test loss: 3.332376218694398\n",
      "Epoch EM: 58.01324503311258\n",
      "Epoch F1: 69.31268368469446\n",
      "====================================================================================\n",
      "Epoch 7\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.3311954019487655| Time: 5m 58s\n",
      "Epoch test loss: 3.388809747356126\n",
      "Epoch EM: 57.96594134342479\n",
      "Epoch F1: 69.45097803617494\n",
      "====================================================================================\n",
      "Epoch 8\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.19540534510358| Time: 6m 4s\n",
      "Epoch test loss: 3.4353418929444546\n",
      "Epoch EM: 57.92809839167455\n",
      "Epoch F1: 69.75327375503095\n",
      "====================================================================================\n",
      "Epoch 9\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 2.0506471216766986| Time: 5m 55s\n",
      "Epoch test loss: 3.5278110691431213\n",
      "Epoch EM: 58.15515610217597\n",
      "Epoch F1: 69.49068457964582\n",
      "====================================================================================\n",
      "Epoch 10\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.9361666237826345| Time: 6m 3s\n",
      "Epoch test loss: 3.5704355272950705\n",
      "Epoch EM: 57.30368968779565\n",
      "Epoch F1: 69.00993511486486\n",
      "====================================================================================\n",
      "Epoch 11\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.8335746746093278| Time: 5m 44s\n",
      "Epoch test loss: 3.689223484929059\n",
      "Epoch EM: 57.48344370860927\n",
      "Epoch F1: 69.1731435831521\n",
      "====================================================================================\n",
      "Epoch 12\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.727827857453528| Time: 5m 45s\n",
      "Epoch test loss: 3.767117672474566\n",
      "Epoch EM: 56.9914853358562\n",
      "Epoch F1: 68.86129652890855\n",
      "====================================================================================\n",
      "Epoch 13\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.6365158617098832| Time: 5m 54s\n",
      "Epoch test loss: 3.885732874933101\n",
      "Epoch EM: 56.688741721854306\n",
      "Epoch F1: 68.43403215721884\n",
      "====================================================================================\n",
      "Epoch 14\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.546902778222218| Time: 5m 53s\n",
      "Epoch test loss: 3.9781219610210194\n",
      "Epoch EM: 56.546830652790916\n",
      "Epoch F1: 68.50947441608167\n",
      "====================================================================================\n",
      "Epoch 15\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.466434994539877| Time: 5m 39s\n",
      "Epoch test loss: 4.044775535251773\n",
      "Epoch EM: 56.253547776726585\n",
      "Epoch F1: 68.1814537267222\n",
      "====================================================================================\n",
      "Epoch 16\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.405568330911153| Time: 5m 48s\n",
      "Epoch test loss: 4.042681035113469\n",
      "Epoch EM: 56.22516556291391\n",
      "Epoch F1: 68.4682492168954\n",
      "====================================================================================\n",
      "Epoch 17\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.3357809093636526| Time: 5m 47s\n",
      "Epoch test loss: 4.1764433948868085\n",
      "Epoch EM: 56.688741721854306\n",
      "Epoch F1: 68.47350828458809\n",
      "====================================================================================\n",
      "Epoch 18\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train loss : 1.2745846578584856| Time: 6m 6s\n",
      "Epoch test loss: 4.269512704040011\n",
      "Epoch EM: 56.62251655629139\n",
      "Epoch F1: 68.61522333616338\n",
      "====================================================================================\n",
      "Epoch 19\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.2163181104776681| Time: 6m 5s\n",
      "Epoch test loss: 4.410901573561097\n",
      "Epoch EM: 55.83727530747398\n",
      "Epoch F1: 67.99605297123192\n",
      "====================================================================================\n",
      "Epoch 20\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.1582625538602283| Time: 6m 4s\n",
      "Epoch test loss: 4.566510242386348\n",
      "Epoch EM: 55.36423841059602\n",
      "Epoch F1: 67.54538726787347\n",
      "====================================================================================\n",
      "Epoch 21\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.10830620181967| Time: 6m 1s\n",
      "Epoch test loss: 4.578125541123823\n",
      "Epoch EM: 55.88457899716178\n",
      "Epoch F1: 68.14933006194383\n",
      "====================================================================================\n",
      "Epoch 22\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.0666182733453629| Time: 5m 14s\n",
      "Epoch test loss: 4.704341205693816\n",
      "Epoch EM: 56.168401135288555\n",
      "Epoch F1: 67.8416505687233\n",
      "====================================================================================\n",
      "Epoch 23\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 1.0300516759365068| Time: 5m 12s\n",
      "Epoch test loss: 4.632684002534915\n",
      "Epoch EM: 55.27909176915799\n",
      "Epoch F1: 67.68615085916021\n",
      "====================================================================================\n",
      "Epoch 24\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.9834232174710621| Time: 5m 9s\n",
      "Epoch test loss: 4.833732158982967\n",
      "Epoch EM: 55.46830652790918\n",
      "Epoch F1: 67.57700269846539\n",
      "====================================================================================\n",
      "Epoch 25\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.9483428347422248| Time: 5m 7s\n",
      "Epoch test loss: 4.883864732194597\n",
      "Epoch EM: 55.629139072847686\n",
      "Epoch F1: 67.85538646683902\n",
      "====================================================================================\n",
      "Epoch 26\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.9123007657159647| Time: 5m 6s\n",
      "Epoch test loss: 4.996539752968952\n",
      "Epoch EM: 55.487228003784296\n",
      "Epoch F1: 67.46842133779363\n",
      "====================================================================================\n",
      "Epoch 27\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.8757547772233357| Time: 5m 7s\n",
      "Epoch test loss: 5.02500031589901\n",
      "Epoch EM: 55.47776726584674\n",
      "Epoch F1: 67.83783831807996\n",
      "====================================================================================\n",
      "Epoch 28\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.8417583681654055| Time: 5m 7s\n",
      "Epoch test loss: 5.162377953333105\n",
      "Epoch EM: 55.175023651844846\n",
      "Epoch F1: 67.57019137099239\n",
      "====================================================================================\n",
      "Epoch 29\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.810343089445133| Time: 5m 7s\n",
      "Epoch test loss: 5.077980840026716\n",
      "Epoch EM: 55.29801324503311\n",
      "Epoch F1: 67.7493216547528\n",
      "====================================================================================\n",
      "Epoch 30\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting batch: 1000\n",
      "Starting batch: 1500\n",
      "Starting batch: 2000\n",
      "Starting batch: 2500\n",
      "Starting Testing .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Epoch train loss : 0.7823620786913861| Time: 5m 6s\n",
      "Epoch test loss: 5.267637802987085\n",
      "Epoch EM: 55.25070955534532\n",
      "Epoch F1: 67.43041800171635\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    test_loss, em, f1 = test(model, test_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch test loss: {test_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "435df4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), './model/model_stacked_bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbbd9096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentReader(\n",
       "  (context_bilstm): StackedBiLSTM(\n",
       "    (lstms): ModuleList(\n",
       "      (0): LSTM(600, 128, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       "  (question_bilstm): StackedBiLSTM(\n",
       "    (lstms): ModuleList(\n",
       "      (0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (2): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       "  (glove_embedding): Embedding(110474, 300)\n",
       "  (align_embedding): AlignQuestionEmbedding(\n",
       "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (linear_attn_question): LinearAttentionLayer(\n",
       "    (linear): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       "  (bilinear_attn_start): BilinearAttentionLayer(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (bilinear_attn_end): BilinearAttentionLayer(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "HIDDEN_DIM = 128\n",
    "EMB_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_DIRECTIONS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model_copy = DocumentReader(HIDDEN_DIM,\n",
    "                       EMB_DIM, \n",
    "                       NUM_LAYERS, \n",
    "                       NUM_DIRECTIONS, \n",
    "                       DROPOUT, \n",
    "                       device)\n",
    "model_copy.load_state_dict(torch.load('./model/model_stacked_bilstm.h5'))\n",
    "model_copy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753edb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
